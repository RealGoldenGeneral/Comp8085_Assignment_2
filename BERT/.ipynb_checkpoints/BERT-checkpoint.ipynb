{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e49d686d-b75e-406a-8c71-28e2dddc06a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Store the dataset and subsample it\n",
    "import pandas as pd\n",
    "chunk_array = []\n",
    "with pd.read_json(\"../yelp_academic_dataset_review.json\", orient=\"records\", lines=True, chunksize=40000) as reader:\n",
    "    for chunk in reader:\n",
    "        chunk_array.append(chunk)\n",
    "raw_dataset = chunk_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e56f7b0-4e1f-4d15-bb91-23d23ee45c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    review_id                 user_id             business_id  \\\n",
      "0      KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA  XQfwVwDr-v0ZS3_CbbE5Xw   \n",
      "1      BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q  7ATYjTIgM3jUlt4UM3IypQ   \n",
      "2      saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A  YjUWPpI6HXG530lwP-fb2A   \n",
      "3      AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA   \n",
      "4      Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ   \n",
      "...                       ...                     ...                     ...   \n",
      "39995  EYkouQA9oWuiWDNZsYl7aA  WfiilB5OXV7vSmHP-80n-A  HQ-C47_Xi5it1KzwEc0u0A   \n",
      "39996  xZi6gbagKAzqCKjtDLrhGQ  BDEi5eV-uhP4A4atMNzW5w  ena3aLdMz2ym_OPVuTIJ2g   \n",
      "39997  bxEjtoD74xPBJnMtV2759A  GLCcS7HGPa7MD997xq5W9w  34Eqv8jXgxg_EEwcsNgeeg   \n",
      "39998  xT8DOnqIu_7N-9AnkFftaQ  bNnBwW5kNO77KTgMeVhxKg  F2C5ENuY8CXfgoW-gAMdDA   \n",
      "39999  Wy7Njv1S0SaLEk9Bj-ZHPw  ZVREpaL2TPWMtUDJaUZulg  ORL4JE6tz3rJxVqkdKfegA   \n",
      "\n",
      "       stars  useful  funny  cool  \\\n",
      "0          3       0      0     0   \n",
      "1          5       1      0     1   \n",
      "2          3       0      0     0   \n",
      "3          5       1      0     1   \n",
      "4          4       1      0     1   \n",
      "...      ...     ...    ...   ...   \n",
      "39995      4       0      0     0   \n",
      "39996      5       4      1     2   \n",
      "39997      4       1      0     0   \n",
      "39998      3       0      0     0   \n",
      "39999      3       0      0     0   \n",
      "\n",
      "                                                    text                date  \n",
      "0      If you decide to eat here, just be aware it is... 2018-07-07 22:09:11  \n",
      "1      I've taken a lot of spin classes over the year... 2012-01-03 15:28:18  \n",
      "2      Family diner. Had the buffet. Eclectic assortm... 2014-02-05 20:30:30  \n",
      "3      Wow!  Yummy, different,  delicious.   Our favo... 2015-01-04 00:01:03  \n",
      "4      Cute interior and owner (?) gave us tour of up... 2017-01-14 20:54:15  \n",
      "...                                                  ...                 ...  \n",
      "39995  Late night, kind of pricey pizza but good for ... 2015-07-07 19:13:32  \n",
      "39996  Tony Blanche, owner of the Clam Tavern, has op... 2012-01-07 21:46:58  \n",
      "39997  a very interesting change of pace for breakfas... 2016-08-27 15:42:30  \n",
      "39998  This place is just \"OK\" in my book. I'll go if... 2017-05-28 23:58:04  \n",
      "39999  Stayed here for a meeting we had at Vanderbilt... 2014-09-13 04:19:46  \n",
      "\n",
      "[40000 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d7581e3-fd58-45f7-b953-ca2929a577e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Split dataset into train, validation, and test sets\n",
    "# Manually select portion of dataset\n",
    "train_size = 20000\n",
    "val_size = 30000\n",
    "\n",
    "# Manually slice dataset\n",
    "train_df = raw_dataset.iloc[:train_size]\n",
    "val_df = raw_dataset.iloc[train_size:val_size]\n",
    "test_df = raw_dataset.iloc[val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d2b904c-c6ca-4c75-bf29-1c60a7a023b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1970n\\comp8085\\Comp8085_Assignment_2\\project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Part 3: Tokenize the dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "# Tokenize review texts\n",
    "train_texts = train_df['text'].tolist()\n",
    "val_texts = val_df['text'].tolist()\n",
    "test_texts = test_df['text'].tolist()\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db52b062-e89d-420d-bce9-03891e4c680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4: Extract labels\n",
    "import torch\n",
    "\n",
    "columns = ['stars', 'useful', 'cool', 'funny']\n",
    "\n",
    "train_labels = torch.tensor(train_df[columns].values)\n",
    "val_labels = torch.tensor(val_df[columns].values)\n",
    "test_labels = torch.tensor(test_df[columns].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1550c0eb-c4b6-4e82-af9b-d29098e85968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 5: Prepare dataset for PyTorch model\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "# Create custom Dataset class\n",
    "class YelpReviewDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create dataset objects for training, validation, and testing\n",
    "train_dataset = YelpReviewDataset(train_encodings, train_labels)\n",
    "val_dataset = YelpReviewDataset(val_encodings, val_labels)\n",
    "test_dataset = YelpReviewDataset(test_encodings, test_labels)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_loader = DataLoader(train_dataset, batch_size=6, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=6)\n",
    "test_loader = DataLoader(test_dataset, batch_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4d0c68b-abd0-41fd-8b0f-8d5f06ace181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Part 4: Load pre-trained BERT model\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\", num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773420c2-e795-4745-8214-147bc61184ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 1.1920781727962924\n",
      "Epoch 1 - Validation mse: 1.2692443608091875\n",
      "Epoch 2 - Training loss: 0.9796683279664469\n"
     ]
    }
   ],
   "source": [
    "# Part 5: Train the model\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Define loss function (Mean Squared Error for regression)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, labels.float())\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    preds, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            preds.append(logits.cpu().numpy())\n",
    "            true_labels.append(labels.cpu().numpy())\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "    # Evaluate using mean squared error\n",
    "    mse = mean_squared_error(true_labels, preds)\n",
    "    return mse\n",
    "\n",
    "# Run training and evaluation for multiple epochs\n",
    "for epoch in range(2): # Adjust the number of epochs as necessary\n",
    "    train_loss = train_epoch(model, train_loader, loss_fn, optimizer)\n",
    "    print(f\"Epoch {epoch + 1} - Training loss: {train_loss}\")\n",
    "\n",
    "    mse = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch + 1} - Validation mse: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c97fd66-98d9-45cd-82d1-bea80ec02d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on the test set: 1.4481527309236755\n"
     ]
    }
   ],
   "source": [
    "# Part 6: Predict the test set\n",
    "def predict_on_test(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            all_preds.append(logits.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "    # Convert list of predictions and labels to numpy arrays\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    return all_preds, all_labels\n",
    "\n",
    "# Get predictions on the test set\n",
    "test_preds, test_labels = predict_on_test(model, test_loader)\n",
    "\n",
    "mse = mean_squared_error(test_labels, test_preds)\n",
    "print(f'Mean Squared Error on the test set: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a0ebbfd-3d38-4045-8c09-acc40cc22b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This place is fantastic. I was apprehensive (as I often am walking into bike shops) when I walked in, but quickly realized that this place was different. There were some hard core road bikers in the back talking in their own language, a few weekend warriors getting their bikes worked on and getting gear, and me: an aspiring cyclist intimidated by the prices, lingo, and other details of taking up this pastime.  I normally cringe when I walk into bike stores and feel that if I don't drop 4 grand quickly on a carbon fiber bike or show them the yellow jersey I wore when leading three stages of the Tour de France, that I'll quickly be shown the exit (or a brusque cold shoulder).\n",
      "\n",
      "At this place, a guy came up and asked what I was looking for and how he could help. I explained my situation: I have a road bike, I want to start using it as my primary means of transportation and for recreation, and I want to get the rest of the equipment that I need. He quickly explained what equipment is NEEDED versus what equipment is NICE TO HAVE. We went through helmets, floor pumps, bike shorts, bike shoes, flat kits, etc. I already have shoes and pedals as well as some tire repair kits. When I told him that, he quickly moved on - never pressuring me to upgrade or to change anything. We talked tire options (tube liners/extra-thick tires/tube sealant), tune up needs, maintenance suggestions, etc.\n",
      "\n",
      "In the end, I only bought a helmet because I had all of the gear in the NEED category already. As I get going, I'll add the NICE TO HAVE stuff as I see fit. And I'll buy it from Reno Cycling & Fitness because they know how to treat customers. So it is amazing, for a fairly minor purchase, this guy invested nearly half an hour talking to me - never talking down to me. He understood my needs, listened when i talked and helped me immensely.\n",
      "\n",
      "In a world of diminishing customer service, this place stands out. The depth of knowledge available will not be found in a big box store or at an online discounter. Keep honest, hardworking local companies in business and help yourself in the process.\n",
      "True labels: [5 2 1 0]\n",
      "Predicted labels: [4.200233  3.868287  1.6022439 1.2631226]\n",
      "--------------------------------------------------\n",
      "Review: I did not enjoy my food here. The fish was fried so hard I couldn't eat it. My rice taste like some boil n the bag rice. I was not impressed at all.\n",
      "True labels: [1 1 0 0]\n",
      "Predicted labels: [1.1512867  0.58623976 0.08035312 0.18284145]\n",
      "--------------------------------------------------\n",
      "Review: The pho is EXCELLENT and so is the service...but it's not the tidiest place in town.  The walls are pretty dirty, which is surprising for a newish place, and the floors are often not clean.  Regardless, it's still my go to pho spot.\n",
      "True labels: [4 0 0 0]\n",
      "Predicted labels: [3.3426218  0.8555436  0.23942405 0.15725514]\n",
      "--------------------------------------------------\n",
      "Review: Cafe deluxe was awesome. This used to be a laundromat back in the day, and now they've established a quirky little breakfast spot. We got some kind of breakfast scramble with meats and various other veggie ingredients. We ordered some flour tortillas on the side which were huge. \n",
      "\n",
      "The egg scrambles were huge portions, so after we got our fill, I made us two huge burritos to go. \n",
      "\n",
      "Cool place, nice people, and great coffee.\n",
      "True labels: [4 1 1 0]\n",
      "Predicted labels: [4.727475   1.2731532  0.63131756 0.32290602]\n",
      "--------------------------------------------------\n",
      "Review: I recently stayed at The Saint with my sister for a few nights and loved it.  The minute we walked in we knew we'd loved it because of the amazing decor and the amazing hospitality. Right from the first time we walked in, we felt so welcome with the doorman and the front desk girls. SUPER!!! FRIENDLY!!! \n",
      "\n",
      "The hotel lobby and entrance all looked cool with great paintings and decor.  Just like their other locations, they have these oversized chairs that are great for taking fun pictures so before we even went up to see our room we already took a ton of cool pictures.  The chairs make you feel so miniature.\n",
      "\n",
      "The hotel also has a really nice bar in the lobby.  It's decorated with lots of red chandeliers hanging over the bar and they serve some really great drinks.  If you're ever there, you need to make sure you go for there happy hours drinks (WAY cheaper).  At night the bar does get pretty busy and every night they have a live band so it was nice to sit and listen to them but it does get a little loud.\n",
      "\n",
      "When we finally decided to go up to our room, we entered the elevator and we were so surprised and thought it was funny that they have a television that is always playing Classic Jazz (how cool is that?).  When the elevator doors open, the entire hallway was all dark with  blue lights; at first it reminded me of the movie The Shinning LOL\n",
      "\n",
      "Our room was really roomy and nice.  At first I didn't like that our room was at the back of the building where you have no view (just looking at another building right there) but later i was happy to be in the back because when you get the rooms along Canal Street it is really loud with all the traffic and street action; the view wasn't worth it so we kept our back room.  But I will say I was so happy that the hotel was willing to change us if we wanted.\n",
      "\n",
      "The location was so good; really close to everything we needed like best spot to get the trolleys, on the edge of the French Quarter, nearby so much more.\n",
      "True labels: [5 5 4 3]\n",
      "Predicted labels: [4.591026   2.0064812  0.915436   0.42695925]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Review: {raw_dataset['text'].iloc[30000 + i]}\")\n",
    "    print(f\"True labels: {test_labels[i]}\")\n",
    "    print(f\"Predicted labels: {test_preds[i]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "278fb291-6820-4e46-a347-de28f2091f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    review_id                 user_id             business_id  \\\n",
      "40000  zwu3HkuLQW0udgNb43e-dw  CigyryBCd5GFc01FnXvgcQ  u7_3L1NBWgxhBM_B-cmmnA   \n",
      "40001  2TkFR7TG8TUzfCMP1oiwJQ  PWVL8fYmjBHqoyjt6LIcpA  vN6v8m4DO45Z4pp8yxxF_w   \n",
      "40003  clSTQLuLu11UhwSPKEtToA  pTw5mqWvjJzbe23GTFnwSg  ltBBYdNzkeKdCNPDAsxwAA   \n",
      "40004  kBuiwIzGu_4G3VXNfH3Bow  w4he3nb2wQI5h5rTbCaqXA  OINbC0rpDVJ5bfxt3LO9fw   \n",
      "40007  xo5Qxcq-8Xi9m9NmehXBlw  2iAk0uFpg0aAyA2C1M0f7g  B6Lvq3sOYwhKxPzadDka9g   \n",
      "...                       ...                     ...                     ...   \n",
      "51216  xuZdKY4G2mjZL6E8Oej30w  A7io7WGZA1p0Q3_Mp9vGdw  2pXrwBssKTp30uxbQgl6kw   \n",
      "51217  459NXCRcGRprImr9gYQO7A  9-FkHuGRbzfUjqPGbNv3Jg  kd0Sv_ZjnBhbMxs0U4KgQw   \n",
      "51220  kv9FV-etjnysLXLUazOoGw  QEMfgXbtAU-gHIDyfocB-w  P8X2NUCEQm-YPTS3em1Kgg   \n",
      "51221  XDrklSZWRW6qqu8ydTlXgA  JzQ4fIrplO-o6sjIgy1EsA  t1qF12NdW8KvCqxqbvy-Hg   \n",
      "51222  OD6FA4WdW845bVrQTAwIyA  bz8HMDpRYdDmzqAKepbW5A  6bFx8j2KqPfmT0XEUS1RMg   \n",
      "\n",
      "       stars  useful  funny  cool  \\\n",
      "40000      4       5      0     2   \n",
      "40001      4       0      0     0   \n",
      "40003      5       0      0     0   \n",
      "40004      4       0      0     0   \n",
      "40007      5       2      0     1   \n",
      "...      ...     ...    ...   ...   \n",
      "51216      5       0      1     0   \n",
      "51217      4       0      0     0   \n",
      "51220      3       3      0     0   \n",
      "51221      4       6      5     1   \n",
      "51222      5       3      0     0   \n",
      "\n",
      "                                                    text                date  \n",
      "40000  I was unexpectedly surprised with how much I l... 2017-08-20 00:54:11  \n",
      "40001  It was a wait,but worth it I enjoy every bite\\... 2012-09-14 12:51:37  \n",
      "40003  Excellent service, has air conditioning which ... 2018-05-11 21:48:59  \n",
      "40004  Came here for Sunday brunch, of course the pla... 2016-08-28 16:46:50  \n",
      "40007  AMAZING. You need to make a reservation far in... 2012-11-11 04:52:48  \n",
      "...                                                  ...                 ...  \n",
      "51216  Im not really that much of a beach person. I, ... 2016-06-23 16:20:25  \n",
      "51217  I have eaten here 3 separate times and the goo... 2015-06-09 04:12:24  \n",
      "51220  I went out last week on the 1/2 day trip.  abo... 2011-10-29 16:45:55  \n",
      "51221  I'd also like to boost this place and clear up... 2008-12-09 01:21:11  \n",
      "51222  I made an appointment for early Saturday morni... 2015-10-17 14:37:33  \n",
      "\n",
      "[10000 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# Part 7: Setup dataset for experiment 2\n",
    "exp_2_df = chunk_array[1]\n",
    "exp_2_df = exp_2_df.loc[exp_2_df['stars'] != 1]\n",
    "exp_2_df = exp_2_df.iloc[:10000]\n",
    "print(exp_2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03bf8e34-8572-497e-86f9-00261f9c37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 8: Prepare dataset\n",
    "exp_2_texts = exp_2_df['text'].to_list()\n",
    "exp_2_encodings = tokenizer(exp_2_texts, truncation=True, padding=True, max_length=512)\n",
    "exp_2_labels = torch.tensor(exp_2_df[columns].values)\n",
    "exp_2_dataset = YelpReviewDataset(exp_2_encodings, exp_2_labels)\n",
    "exp_2_loader = DataLoader(exp_2_dataset, batch_size = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af8ddad2-c09a-498e-bcc2-ce4c8fed8509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on the exp_2 set: 1.458718695466033\n"
     ]
    }
   ],
   "source": [
    "# Part 9: Get predictions on the experiment 2 set\n",
    "exp_2_preds, exp_2_labels = predict_on_test(model, exp_2_loader)\n",
    "\n",
    "mse = mean_squared_error(exp_2_labels, exp_2_preds)\n",
    "print(f'Mean Squared Error on the exp_2 set: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae083c2e-cacf-42ae-90b0-1b3886ca8c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 10: Setup dataset for experiment 3\n",
    "count_text = []\n",
    "from collections import Counter\n",
    "exp_3_df = chunk_array[2]\n",
    "exp_3_texts = exp_3_df['text'].to_list()\n",
    "# for text in exp_3_texts:\n",
    "#     count_text = count_text + text.split()\n",
    "# counter = Counter(count_text)\n",
    "# most_common = counter.most_common(1)\n",
    "# print(most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1f71f88-88ed-4d0f-8df9-05b1ae3d70da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     review_id                 user_id  \\\n",
      "80006   2TtzJxP31jOa1ihySAWwwQ  skXKKYvpaBAnJ2vH0OytVw   \n",
      "80008   74Rx2JEsXELR5QmwvDzThQ  Rvfa3lFgK0W91AvfLrGhsg   \n",
      "80015   r0HqL29dDeNSP6KCJ5LJJg  8J2CZaOR1mt2UPZzoOlIGg   \n",
      "80022   RLB0K2WviFdK9dbNk4_ewg  EiiAascIEQmYcWI6BQe0tg   \n",
      "80026   wD4iajcnFGh0F--xsf7eeA  TRzbT9POo8TswwyTigIQDQ   \n",
      "...                        ...                     ...   \n",
      "119956  gGcu1WJ3B7iK2kH2q7C_Fg  mJatJshTyF_hpWgYd2dY3A   \n",
      "119959  _F-VXZc8L_TPccBs0S_kaQ  TZwKmfmbzevOtOoZhlZVxA   \n",
      "119971  zgC0VCvpBfJzXCVZCRuRbw  BCpRqfCzNr4rJy00t3OOCw   \n",
      "119982  fmKgMjWGpSFKKDl5ZqoKsw  3MiBisjFH9mNKUgzA0x_RA   \n",
      "119999  IJzJrH-WrBsyVJzveIKOVw  28H58l9RTUCxRntOTwe0aQ   \n",
      "\n",
      "                   business_id  stars  useful  funny  cool  \\\n",
      "80006   ugSj0rWlWQ57-FuKuieGXA      2       0      0     0   \n",
      "80008   JvawJ9bSr22xn4R9oLvl_w      5       0      0     0   \n",
      "80015   -wB5H63ERJ9S0oCp_ULR0Q      5       1      0     0   \n",
      "80022   -mq1DwgcLU96PQbmcv3jRQ      4       1      0     0   \n",
      "80026   Q-prSTdggNlxAEFV88BZOw      5       0      0     0   \n",
      "...                        ...    ...     ...    ...   ...   \n",
      "119956  196CWwMAtAcA21jYiMyRzg      5       0      0     0   \n",
      "119959  iJOwwuhHP_0BCHmpmJ8l4Q      2       0      5     0   \n",
      "119971  qppGA9vCUPfw9ngOy_R90w      4       0      0     0   \n",
      "119982  x03j48hmv0R7NQcUjD0MQg      4       0      0     0   \n",
      "119999  AaWmckcT3O1iQSAIUmzwUA      1       1      0     0   \n",
      "\n",
      "                                                     text                date  \n",
      "80006   The staff is attentive and friendly. The pizza... 2017-10-13 00:40:32  \n",
      "80008   Service was prime time.  Thanks kiki. Oysters ... 2016-04-25 21:13:12  \n",
      "80015   Quaint Conor Bagel shop. Family Owned great co... 2017-12-31 14:11:05  \n",
      "80022   Great food - horrible service! Panko Grouper, ... 2013-07-19 11:33:14  \n",
      "80026   My cousin and I have decided to make this our ... 2013-07-18 17:56:42  \n",
      "...                                                   ...                 ...  \n",
      "119956  The Shrimp Creole alone makes this 5stars. Qui... 2015-08-10 02:09:51  \n",
      "119959  I looked all around\\n\\n             and\\n\\n   ... 2011-05-01 21:18:23  \n",
      "119971  Great wines, bites, and flights. The place is ... 2014-07-17 00:06:13  \n",
      "119982  Yuuuummy. That was really good. But I think it... 2018-05-12 09:07:23  \n",
      "119999  Everything about this place fucking sucks. Pan... 2016-08-16 15:48:31  \n",
      "\n",
      "[3093 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "exp_3_df = exp_3_df[exp_3_df['text'].str.contains(\"the\", case=True) == False]\n",
    "print(exp_3_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "616a4aa6-0a1f-4f7e-9f9f-b84309f04cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_3_texts = exp_3_df['text'].to_list()\n",
    "exp_3_encodings = tokenizer(exp_3_texts, truncation=True, padding=True, max_length=512)\n",
    "exp_3_labels = torch.tensor(exp_3_df[columns].values)\n",
    "exp_3_dataset = YelpReviewDataset(exp_3_encodings, exp_3_labels)\n",
    "exp_3_loader = DataLoader(exp_3_dataset, batch_size = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a20d0434-7561-4192-a94e-b8fdc2f15c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on the exp_3 set: 0.668613642903321\n"
     ]
    }
   ],
   "source": [
    "exp_3_preds, exp_3_labels = predict_on_test(model, exp_3_loader)\n",
    "\n",
    "mse = mean_squared_error(exp_3_labels, exp_3_preds)\n",
    "print(f'Mean Squared Error on the exp_3 set: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9667828b-a789-4241-bdd8-0fdab8af014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('BERT.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e4889f5-4aec-4c8c-bf83-2959e2843d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_json(\"bert_test_set.json\", orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ff4755-50f2-422a-aec4-8580889dff6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
